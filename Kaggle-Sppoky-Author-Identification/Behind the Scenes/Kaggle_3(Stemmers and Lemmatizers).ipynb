{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['EAP' 'HPL' 'MWS']\n",
      "[0 1 2]\n",
      "19579\n",
      "19579\n",
      "8392\n",
      "['this process, however, afford me no mean of ascertain the dimens of my dungeon; as i might make it circuit, and return to the point whenc i set out, without be awar of the fact; so perfect uniform seem the wall.', 'it never onc occur to me that the fumbl might be a mere mistake.']\n",
      "['still, as i urg our leav ireland with such inquietud and impatience, my father thought it best to yield.', 'if a fire want fanning, it could readili be fan with a newspaper, and as the govern grew weaker, i have no doubt that leather and iron acquir durabl in proportion, for, in a veri short time, there was not a pair of bellow in all rotterdam that ever stood in need of a stitch or requir the assist of a hammer.']\n",
      "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.4, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None)\n",
      "[2 0 1 0 0 0 0 2 0 0]\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn.svm import SVC\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "data = pd.read_csv('train.csv', sep=',', header=None)\n",
    "test = pd.read_csv('test.csv', sep=',', header=None)\n",
    "\n",
    "labels = data[2]\n",
    "le = preprocessing.LabelEncoder()\n",
    "y_train = le.fit_transform(labels)\n",
    "print (le.classes_)\n",
    "print (le.transform(le.classes_))\n",
    "\n",
    "\n",
    "train_text = list(data[1])\n",
    "test_text = list(test[1])\n",
    "\n",
    "def parseOutText(f):\n",
    "    content = f\n",
    "\n",
    "    words = \"\"\n",
    "    if len(content) > 1:\n",
    "        stemmer = SnowballStemmer(\"english\")\n",
    "        split = content.split()  \n",
    "        text = [stemmer.stem(word) for word in split]\n",
    "        words = ' '.join(text)\n",
    "        \n",
    "    return words\n",
    "\n",
    "X_train = []\n",
    "for sentence in train_text:\n",
    "    stemmed_sentence = parseOutText(sentence)\n",
    "    X_train.append(stemmed_sentence)\n",
    "X_test = []\n",
    "for sentencee in test_text:\n",
    "    stemmed_sentencee = parseOutText(sentencee)\n",
    "    X_test.append(stemmed_sentencee)\n",
    "\n",
    "print (len(X_train))\n",
    "print (len(y_train))\n",
    "print (len(X_test))\n",
    "print (X_train[:2])\n",
    "print (X_test[:2])\n",
    "\n",
    "tfidff = TfidfVectorizer(max_df=0.4)\n",
    "print (tfidff)\n",
    "\n",
    "t_train_text = tfidff.fit_transform(X_train)\n",
    "\n",
    "t_test_test = tfidff.transform(X_test)\n",
    "\n",
    "tfidff.vocabulary_.__len__()\n",
    "\n",
    "clfmnb = MultinomialNB(alpha=0.02)\n",
    "clfmnb.fit(t_train_text, y_train)\n",
    "\n",
    "predicted = clfmnb.predict_proba(t_test_test)\n",
    "\n",
    "pred = clfmnb.predict(t_test_test)\n",
    "\n",
    "print (pred[:10])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = le.inverse_transform(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MWS' 'EAP' 'HPL' 'EAP' 'EAP' 'EAP' 'EAP' 'MWS' 'EAP' 'EAP']\n"
     ]
    }
   ],
   "source": [
    "print (pred[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "data_id = pd.DataFrame(test[0])\n",
    "data_text = pd.DataFrame(test[1])\n",
    "pred = pd.DataFrame(pred)\n",
    "\n",
    "data_text = data_text.rename(columns={0:'text'})\n",
    "pred = pred.rename(columns={0:'author'})\n",
    "data_id = data_id.rename(columns={0:'id'})\n",
    "\n",
    "output = pd.concat([data_id,data_text,pred],axis=1)\n",
    "\n",
    "output.to_csv('output_with_author.csv', sep=',', index=False)\n",
    "print ('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_id = pd.DataFrame(test[0])\n",
    "pred_proba = pd.DataFrame(predicted)\n",
    "\n",
    "pred_proba = pred_proba.rename(columns={0:'EAP',1:'HPL',2:'MWS'})\n",
    "data_id = data_id.rename(columns={0:'id'})\n",
    "\n",
    "output = pd.concat([data_id,pred_proba],axis=1)\n",
    "\n",
    "output.to_csv('submission.csv', sep=',', index=False)\n",
    "print ('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\cpads\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn.svm import SVC\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "nltk.download('wordnet')\n",
    "\n",
    "train_file = pd.read_csv('train.csv', sep=',', header=None)\n",
    "test_file = pd.read_csv('test.csv', sep=',', header=None)\n",
    "\n",
    "X = list(train_file[1])\n",
    "X_test = list(test_file[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "car\n"
     ]
    }
   ],
   "source": [
    "labels = train_file[2]\n",
    "le = preprocessing.LabelEncoder()\n",
    "y = le.fit_transform(labels)\n",
    "\n",
    "lem = WordNetLemmatizer()\n",
    "print (lem.lemmatize('car'))\n",
    "\n",
    "def parseOutText(f):\n",
    "    content = f\n",
    "\n",
    "    words = \"\"\n",
    "    if len(content) > 1:\n",
    "        lem = WordNetLemmatizer()\n",
    "        split = content.split()  \n",
    "        text = [lem.lemmatize(word) for word in split]\n",
    "        words = ' '.join(text)\n",
    "        \n",
    "    return words\n",
    "\n",
    "l_X = []\n",
    "for sentence in X:\n",
    "    lem_sentence = parseOutText(sentence)\n",
    "    l_X.append(lem_sentence)\n",
    "\n",
    "l_Y = []\n",
    "for s in X_test:\n",
    "    lems = parseOutText(s)\n",
    "    l_Y.append(lems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(ngram_range=(1,2), max_df=0.4)\n",
    "\n",
    "t_x_train = tfidf.fit_transform(l_X)\n",
    "\n",
    "t_x_test = tfidf.transform(l_Y)\n",
    "\n",
    "mnb = MultinomialNB(alpha=0.06)\n",
    "\n",
    "mnb.fit(t_x_train, y)\n",
    "\n",
    "pred = mnb.predict_proba(t_x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "data_id2 = pd.DataFrame(test_file[0])\n",
    "\n",
    "pred_proba2 = pd.DataFrame(pred)\n",
    "\n",
    "pred_proba2 = pred_proba2.rename(columns={0:'EAP',1:'HPL',2:'MWS'})\n",
    "data_id2 = data_id2.rename(columns={0:'id'})\n",
    "\n",
    "output2 = pd.concat([data_id2,pred_proba2],axis=1)\n",
    "\n",
    "output2.to_csv('submission.csv', sep=',', index=False)\n",
    "print ('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['EAP' 'HPL' 'MWS']\n",
      "[0 1 2]\n",
      "19579\n",
      "19579\n",
      "8392\n",
      "['thi process, however, afford me no mean of ascertain the dimens of my dungeon; as I might make it circuit, and return to the point whenc I set out, without be awar of the fact; so perfectli uniform seem the wall.', 'It never onc occur to me that the fumbl might be a mere mistake.']\n",
      "['still, as I urg our leav ireland with such inquietud and impatience, my father thought it best to yield.', 'If a fire want fanning, it could readili be fan with a newspaper, and as the govern grew weaker, I have no doubt that leather and iron acquir durabl in proportion, for, in a veri short time, there wa not a pair of bellow in all rotterdam that ever stood in need of a stitch or requir the assist of a hammer.']\n",
      "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.4, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None)\n",
      "[[ 0.18212021  0.03826608  0.7796137 ]\n",
      " [ 0.91894812  0.05273575  0.02831613]\n",
      " [ 0.30600451  0.67806656  0.01592893]\n",
      " [ 0.60694722  0.38825739  0.00479539]\n",
      " [ 0.7521813   0.12861853  0.11920017]\n",
      " [ 0.67955761  0.26990889  0.05053351]\n",
      " [ 0.63116787  0.21715372  0.15167841]\n",
      " [ 0.07229178  0.18311972  0.7445885 ]\n",
      " [ 0.9585424   0.03840614  0.00305146]\n",
      " [ 0.65746125  0.13146779  0.21107096]]\n"
     ]
    }
   ],
   "source": [
    "#Bekar hai niche wala\n",
    "from time import time\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn.svm import SVC\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "data = pd.read_csv('train.csv', sep=',', header=None)\n",
    "test = pd.read_csv('test.csv', sep=',', header=None)\n",
    "\n",
    "labels = data[2]\n",
    "le = preprocessing.LabelEncoder()\n",
    "y_train = le.fit_transform(labels)\n",
    "print (le.classes_)\n",
    "print (le.transform(le.classes_))\n",
    "\n",
    "\n",
    "train_text = list(data[1])\n",
    "test_text = list(test[1])\n",
    "\n",
    "def parseOutText(f):\n",
    "    content = f\n",
    "\n",
    "    words = \"\"\n",
    "    if len(content) > 1:\n",
    "        stemmer = PorterStemmer()\n",
    "        split = content.split()  \n",
    "        text = [stemmer.stem(word) for word in split]\n",
    "        words = ' '.join(text)\n",
    "        \n",
    "    return words\n",
    "\n",
    "X_train = []\n",
    "for sentence in train_text:\n",
    "    stemmed_sentence = parseOutText(sentence)\n",
    "    X_train.append(stemmed_sentence)\n",
    "X_test = []\n",
    "for sentencee in test_text:\n",
    "    stemmed_sentencee = parseOutText(sentencee)\n",
    "    X_test.append(stemmed_sentencee)\n",
    "\n",
    "print (len(X_train))\n",
    "print (len(y_train))\n",
    "print (len(X_test))\n",
    "print (X_train[:2])\n",
    "print (X_test[:2])\n",
    "\n",
    "tfidff = TfidfVectorizer(max_df=0.4)\n",
    "print (tfidff)\n",
    "\n",
    "t_train_text = tfidff.fit_transform(X_train)\n",
    "\n",
    "t_test_test = tfidff.transform(X_test)\n",
    "\n",
    "tfidff.vocabulary_.__len__()\n",
    "\n",
    "clfmnb = MultinomialNB(alpha=0.06)\n",
    "clfmnb.fit(t_train_text, y_train)\n",
    "\n",
    "predicted = clfmnb.predict_proba(t_test_test)\n",
    "\n",
    "print (predicted[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "data_id = pd.DataFrame(test[0])\n",
    "pred_proba = pd.DataFrame(predicted)\n",
    "\n",
    "pred_proba = pred_proba.rename(columns={0:'EAP',1:'HPL',2:'MWS'})\n",
    "data_id = data_id.rename(columns={0:'id'})\n",
    "\n",
    "output = pd.concat([data_id,pred_proba],axis=1)\n",
    "\n",
    "output.to_csv('submission.csv', sep=',', index=False)\n",
    "print ('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['EAP' 'HPL' 'MWS']\n",
      "[0 1 2]\n",
      "19579\n",
      "19579\n",
      "8392\n",
      "['this process, however, afford me no mean of ascertain the dimens of my dungeon; as i might make it circuit, and return to the point whenc i set out, without be awar of the fact; so perfect uniform seem the wall.', 'it never onc occur to me that the fumbl might be a mere mistake.']\n",
      "['still, as i urg our leav ireland with such inquietud and impatience, my father thought it best to yield.', 'if a fire want fanning, it could readili be fan with a newspaper, and as the govern grew weaker, i have no doubt that leather and iron acquir durabl in proportion, for, in a veri short time, there was not a pair of bellow in all rotterdam that ever stood in need of a stitch or requir the assist of a hammer.']\n",
      "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.4, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None)\n",
      "[[ 0.22972401  0.          0.77027599]\n",
      " [ 0.69241352  0.25091262  0.05667387]\n",
      " [ 0.          0.93391944  0.06608056]\n",
      " [ 0.56220586  0.43779414  0.        ]\n",
      " [ 0.34814001  0.20928404  0.44257595]\n",
      " [ 0.72361002  0.27638998  0.        ]\n",
      " [ 1.          0.          0.        ]\n",
      " [ 0.          0.59760907  0.40239093]\n",
      " [ 1.          0.          0.        ]\n",
      " [ 0.78674867  0.          0.21325133]]\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn.svm import SVC\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn import linear_model\n",
    "\n",
    "data = pd.read_csv('train.csv', sep=',', header=None)\n",
    "test = pd.read_csv('test.csv', sep=',', header=None)\n",
    "\n",
    "labels = data[2]\n",
    "le = preprocessing.LabelEncoder()\n",
    "y_train = le.fit_transform(labels)\n",
    "print (le.classes_)\n",
    "print (le.transform(le.classes_))\n",
    "\n",
    "\n",
    "train_text = list(data[1])\n",
    "test_text = list(test[1])\n",
    "\n",
    "def parseOutText(f):\n",
    "    content = f\n",
    "\n",
    "    words = \"\"\n",
    "    if len(content) > 1:\n",
    "        stemmer = SnowballStemmer(\"english\")\n",
    "        split = content.split()  \n",
    "        text = [stemmer.stem(word) for word in split]\n",
    "        words = ' '.join(text)\n",
    "        \n",
    "    return words\n",
    "\n",
    "X_train = []\n",
    "for sentence in train_text:\n",
    "    stemmed_sentence = parseOutText(sentence)\n",
    "    X_train.append(stemmed_sentence)\n",
    "X_test = []\n",
    "for sentencee in test_text:\n",
    "    stemmed_sentencee = parseOutText(sentencee)\n",
    "    X_test.append(stemmed_sentencee)\n",
    "\n",
    "print (len(X_train))\n",
    "print (len(y_train))\n",
    "print (len(X_test))\n",
    "print (X_train[:2])\n",
    "print (X_test[:2])\n",
    "\n",
    "tfidff = TfidfVectorizer(ngram_range=(1,2), max_df=0.4)\n",
    "print (tfidff)\n",
    "\n",
    "t_train_text = tfidff.fit_transform(X_train)\n",
    "\n",
    "t_test_test = tfidff.transform(X_test)\n",
    "\n",
    "tfidff.vocabulary_.__len__()\n",
    "\n",
    "clfsd = linear_model.SGDClassifier(penalty='l2', alpha=0.0001, loss='modified_huber', l1_ratio=0.5, epsilon=0.1)\n",
    "clfsd.fit(t_train_text, y_train)\n",
    "\n",
    "clfsd_pred = clfsd.predict_proba(t_test_test)\n",
    "\n",
    "print (clfsd_pred[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "data_id = pd.DataFrame(test[0])\n",
    "pred_proba = pd.DataFrame(clfsd_pred)\n",
    "\n",
    "pred_proba = pred_proba.rename(columns={0:'EAP',1:'HPL',2:'MWS'})\n",
    "data_id = data_id.rename(columns={0:'id'})\n",
    "\n",
    "output = pd.concat([data_id,pred_proba],axis=1)\n",
    "\n",
    "output.to_csv('submission.csv', sep=',', index=False)\n",
    "print ('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
